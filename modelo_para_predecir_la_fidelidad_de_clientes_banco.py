# -*- coding: utf-8 -*-
"""Modelo para predecir la fidelidad de clientes banco.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lHJp4uetlZ7bsZK0a8TbqFNrVVsmJ985

# Modelo para predecir la fidelidad clientes banco

## Procesado de datos
"""

#Damos acceso a nuestro google drive
from google.colab import drive
drive.mount('/content/drive')
#Clonamos el repo de github donde están almacenados los datos
!git clone https://github.com/joanby/deeplearning-az.git

#Montamos el dataframe
import pandas as pd

# Ruta al archivo CSV
file_path = '/content/deeplearning-az/original/Part 1 - Artificial Neural Networks (ANN)/Churn_Modelling.csv'

# Cargar el dataset desde el archivo CSV
df = pd.read_csv(file_path)

# Mostrar las primeras filas del dataset
df.head()

# Tratamiento de las variables categoricas

# Seleccionar las columnas categóricas que deseas convertir a variables dummy
columnas_categoricas = ['Geography', 'Gender']  # Reemplaza con los nombres de tus columnas categóricas

# Convertir las columnas categóricas en variables dummy
df_con_dummies = pd.get_dummies(df, columns=columnas_categoricas)

#Eliminamos los campos del dataframe que no nos hacen falta

df_con_dummies = df_con_dummies.drop(columns=['RowNumber', 'CustomerId', 'Surname'])

df_con_dummies.head()

# Escalado de variables
!pip install scikit-learn

# Separamos el dataset en variables dependientes e independientes volcando la info en los arrays X e Y

import numpy as np

# Seleccionar las columnas específicas que deseas volcar a la matriz
# Para no inquirir en la trampa de la colinealidad eliminamos una columna de cada variable dummy
columnas_seleccionadas_X =['CreditScore','Age', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember','EstimatedSalary',	'Geography_Germany',	'Geography_Spain',	'Gender_Male']  # Reemplaza con los nombres de tus columnas
columnas_seleccionadas_Y= ['Exited']
# Volcar los valores de las columnas seleccionadas en una matriz
X = df_con_dummies[columnas_seleccionadas_X].values
y = df_con_dummies[columnas_seleccionadas_Y].values

# Escalado de variables independientes X
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Dividimos los datos en subconjuntos de entrenamiento y test

# Importar las librerías necesarias
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix

# Dividir el conjunto de datos en subconjuntos de entrenamiento y test
# test_size especifica la proporción del conjunto de datos que se utilizará para pruebas
# random_state garantiza la reproducibilidad de los resultados
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Mostrar las dimensiones de los subconjuntos resultantes
print("Dimensiones de X_train:", X_train.shape)
print("Dimensiones de X_test:", X_test.shape)
print("Dimensiones de y_train:", y_train.shape)
print("Dimensiones de y_test:", y_test.shape)

"""## Modelo de Deeplearning"""

# Importar las librerías necesarias
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Definir el modelo
model = Sequential()
model.add(Dense(64, input_dim=len(X_train[0]), activation='relu'))  # Capa de entrada y una capa oculta con 64 neuronas
model.add(Dense(32, activation='relu'))  # Otra capa oculta con 32 neuronas
model.add(Dense(16, activation='relu'))  # Otra capa oculta con 16 neuronas
model.add(Dense(8, activation='relu'))  # Otra capa oculta con 8 neuronas
model.add(Dense(4, activation='relu'))  # Otra capa oculta con 4 neuronas
model.add(Dense(2, activation='relu'))  # Otra capa oculta con 2 neuronas
model.add(Dense(1, activation='sigmoid'))  # Capa de salida con activación sigmoide para salida binaria

# Compilar el modelo
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Entrenar el modelo
model.fit(X_train, y_train, epochs=50, batch_size=10, validation_data=(X_test, y_test))

# Evaluar el modelo
loss, accuracy = model.evaluate(X_test, y_test)
print(f'Pérdida en el conjunto de test: {loss}')
print(f'Precisión en el conjunto de test: {accuracy}')

# Usar el modelo para hacer predicciones
y_pred = model.predict(X_test)
y_pred_prob = y_pred .flatten()  # Probabilidades predichas
y_pred_class = (y_pred_prob > 0.5).astype(int)  # Convertir probabilidades a clases (0 o 1)
conf_matrix = confusion_matrix(y_test, y_pred_class) # Calculamos la matriz de confusión
certidumbre=(conf_matrix [0,0]+conf_matrix [1,1])*100/(conf_matrix [0,0]+conf_matrix [1,0]+conf_matrix [0,1]+conf_matrix [1,1])
print(f'Certidumbre: {certidumbre} %')
# Mostrar algunas predicciones
print(f'Probabilidades predichas: \n {y_pred_prob[:10]}')
print(f'Clases predichas VS clases reales: \n {y_pred_class[:10]} \n {y_test[:10].flatten()}')

""" ## Modelo con K-fold cross validation"""

# Definimos la función create_model() para la creación de modelos a los que hiremos variando las mejores combinaciones de métricas
def create_model():
    model = Sequential()
    model.add(Dense(6, input_dim=len(X_train[0]), activation='sigmoid'))  # Capa de entrada y una capa oculta con 64 neuronas
    model.add(Dense(2, activation='sigmoid'))  # Otra capa oculta con 2 neuronas
    model.add(Dense(1, activation='sigmoid'))  # Capa de salida con activación sigmoide para salida binaria
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

from sklearn.model_selection import KFold
# Definir el número de folds
k = 3
kf = KFold(n_splits=k, shuffle=True, random_state=1)

# Almacenar los resultados de cada fold
accuracies = []

for train_index, val_index in kf.split(X):
    X_train, X_val = X[train_index], X[val_index]
    y_train, y_val = y[train_index], y[val_index]

    # Crear y entrenar el modelo
    model = create_model()
    model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=1)

    # Evaluar el modelo en el conjunto de validación
    loss, accuracy = model.evaluate(X_val, y_val, verbose=1)
    accuracies.append(accuracy)

#Calculamos las estadisticas del modelo
# Calcular la precisión media
mean_accuracy = np.mean(accuracies)
standard_desv=np.std(accuracies)
standard_desv_over_mean=standard_desv*100/mean_accuracy
print(f'Precisión media:{mean_accuracy}\nDesviación típica:{standard_desv} \n% Desviación típica sobre la media:{standard_desv_over_mean} %')